{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Logistic Regression\n",
        "*   Y^ = 1/(1+e^(-z)) and z = w.X+b\n",
        "*   Y_hat = predicted value\n",
        "*   independent variable\n",
        "*   w = weight\n",
        "*   b = bias\n",
        "\n",
        "Gradient Descent\n",
        "it is an  optimisation algorithm used for minimisizing the cost fucntion in various machine learning algorithm . It is used for updating the paramenters of the learning model.\n",
        "\n",
        "w = w - a*dw\n",
        "\n",
        "\n",
        "b = b - a*db\n",
        "\n",
        "Learning rate\n",
        "it is a tuning parameter in an optimisation algorithm that determines the step size at each iteration while moving toward a minimum loss fucntion.\n",
        "\n",
        "dw = 1/m * (Y^-Y).X\n",
        "db = 1/m * (Y^-Y)\n",
        "\n",
        "here dw is the partial derivative of the cost fucntion with respect to weight\n",
        "\n",
        "and db is the partial derivative of the cost fucntion with repect to bias\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rLKkqiDi8ft5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the dependencies"
      ],
      "metadata": {
        "id": "Qhh_YKyx_X7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IAXwRqcmy1IH"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic regresson**"
      ],
      "metadata": {
        "id": "UKzW7hBd_gQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Logistic_Regression():\n",
        "\n",
        "  def __init__(self,learning_rate,no_of_iterations): #common for creating any object like a constructor or an initialiser\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "\n",
        "    #fit function to train the model with some data set\n",
        "  def fit(self,X,Y): #fits the data set to our model\n",
        "    self.m , self.n = X.shape\n",
        "    #number of data points in the dataset(number of rows) --> m\n",
        "    #number of input features in the dataset(number of columns) --> n\n",
        "    #number of weight values is equal to the number of features\n",
        "\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    #implementing gradient descent for optimisation\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_weights\n",
        "\n",
        "  def update_weights(self): #keeps changing the weights and bias values\n",
        "\n",
        "    #Y-hat formul is sigmoid function\n",
        "    Y_hat = 1/(1+np.exp(-(self.X.dot(self.w) + self.b))) #Y_hat is the percentage or probablility of having y as 1\n",
        "      #if Y_hat is 0.2 a 20% chance that output is 1 or 0.2 probablility that output is 1\n",
        "      #the value of Y_hat always lies between 0 and 1\n",
        "\n",
        "    #derivatives\n",
        "    dw = (1/self.m)*np.dot(self.X.T , (Y_hat - self.Y))\n",
        "    db = (1/self.m)*np.sum\n",
        "    #X = [769*8]  X_T = [8*769]\n",
        "    #Y = [769*1]\n",
        "    #updating the weights and bias using gradienet descent\n",
        "    self.w = self.w - self.learning_rate*dw\n",
        "    self.b = self.b - self.learning_rate*db\n",
        "\n",
        "    #sigmoid equation and decision boundary\n",
        "  def predict(self,X): #once you give all the x values it gives 0 or 1 , the final prediction\n",
        "    Y_pred = 1/(1+np.exp(-(  X.dot(self.w) + self.b)))\n",
        "    Y_pred = np.where(Y_pred > 0.5 , 1 , 0)\n",
        "    return Y_pred"
      ],
      "metadata": {
        "id": "qevHB8K9_fip"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work flow of the Logistic regression Model\n",
        "\n",
        "step1: set the learning rate and the number of oterations ; initiate random weight and bias value.\n",
        "\n",
        "step2 : build Logistic Regression Function (sigmoid function )\n",
        "\n",
        "step 3 : update the parameters using gradient descent.\n",
        "\n",
        "and finally we get the best model(besst weight and bias value) as it has the minimum cost fucntion.\n",
        "\n",
        "step 4 : build the predict function to determine the class of the data point.\n"
      ],
      "metadata": {
        "id": "_PQ24_czCQBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Logistic_Regression(learning_rate = 0.01 , no_of_iterations = 1000)\n"
      ],
      "metadata": {
        "id": "WCjPfx8-He42"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
